import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)

plt.plot(x, 1 / (1 + np.exp(-x)), label='Sigmoid')
plt.plot(x, np.tanh(x), label='Tanh')
plt.plot(x, np.maximum(0, x), label='ReLU')

plt.legend()
plt.title("Activation Functions")
plt.grid(True)
plt.show()






import numpy as np
import matplotlib.pyplot as plt

# Activation functions
def sigmoid(x): return 1 / (1 + np.exp(-x))
def relu(x): return np.maximum(0, x)
def tanh(x): return np.tanh(x)
def softmax(x): return np.exp(x) / np.sum(np.exp(x))

# Values for x
x = np.linspace(-10, 10, 100)

# Plot activation functions
plt.figure(figsize=(10, 8))

plt.subplot(2, 2, 1)
plt.plot(x, sigmoid(x), label="Sigmoid")
plt.title("Sigmoid")

plt.subplot(2, 2, 2)
plt.plot(x, relu(x), label="ReLU")
plt.title("ReLU")

plt.subplot(2, 2, 3)
plt.plot(x, tanh(x), label="Tanh")
plt.title("Tanh")

plt.subplot(2, 2, 4)
plt.plot(x, softmax(x - np.max(x)), label="Softmax")  # Softmax requires numerical stability
plt.title("Softmax")

plt.tight_layout()
plt.show()
